{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14da1a6a",
   "metadata": {},
   "source": [
    "# Self-Adaptive Training for Selective Classification\n",
    "\n",
    "This notebook reproduces the SAT paper results on selective classification.\n",
    "\n",
    "**Settings:**\n",
    "- Architecture: VGG-16 with Batch Normalization\n",
    "- Dataset: CIFAR-10\n",
    "- Epochs: 300\n",
    "- SAT Momentum: 0.99\n",
    "- Learning Rate: Decays every 25 epochs by factor of 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b50fef",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b6bc161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu128\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "CUDA device: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# Local imports\n",
    "import models.cifar as models\n",
    "import dataset_utils\n",
    "from loss import SelfAdativeTraining, deep_gambler_loss\n",
    "from utils import AverageMeter, accuracy\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233c3e68",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0c617de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Dataset: cifar10\n",
      "  Architecture: vgg16_bn\n",
      "  Loss: sat\n",
      "  Epochs: 3\n",
      "  SAT Momentum: 0.99\n",
      "  Save directory: ./checkpoints/sat_notebook\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    # Training settings\n",
    "    dataset = 'cifar10'\n",
    "    arch = 'vgg16_bn'\n",
    "    loss_type = 'sat'  # 'sat', 'gambler', or 'ce'\n",
    "    epochs = 3\n",
    "    pretrain_epochs = 0  # Number of epochs to pretrain with cross-entropy\n",
    "    \n",
    "    # Hyperparameters\n",
    "    batch_size_train = 128\n",
    "    batch_size_test = 200\n",
    "    lr = 0.1\n",
    "    momentum = 0.9\n",
    "    sat_momentum = 0.99  # Paper uses 0.99 for selective classification\n",
    "    weight_decay = 5e-4\n",
    "    gamma = 0.5  # LR decay factor\n",
    "    schedule = [25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275]  # LR decay epochs\n",
    "    \n",
    "    # Evaluation settings\n",
    "    expected_coverage = [100., 99., 98., 97., 95., 90., 85., 80., 75., 70., 60., 50., 40., 30., 20., 10.]\n",
    "    reward = 2.2  # For gambler loss\n",
    "    \n",
    "    # System settings\n",
    "    gpu_id = '0'\n",
    "    num_workers = 4\n",
    "    manual_seed = 42\n",
    "    save_dir = './checkpoints/sat_notebook'\n",
    "    \n",
    "config = Config()\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(config.manual_seed)\n",
    "torch.manual_seed(config.manual_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(config.manual_seed)\n",
    "\n",
    "# Set GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = config.gpu_id\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Create save directory\n",
    "os.makedirs(config.save_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Dataset: {config.dataset}\")\n",
    "print(f\"  Architecture: {config.arch}\")\n",
    "print(f\"  Loss: {config.loss_type}\")\n",
    "print(f\"  Epochs: {config.epochs}\")\n",
    "print(f\"  SAT Momentum: {config.sat_momentum}\")\n",
    "print(f\"  Save directory: {config.save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd7e6e4",
   "metadata": {},
   "source": [
    "## 3. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e5edbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset: cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viet2005/workspace/Research/ltr_xai/SAT-Ensemble/venv/lib/python3.12/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  entry = pickle.load(f, encoding=\"latin1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 50000\n",
      "Test samples: 10000\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset\n",
    "print(f'Preparing dataset: {config.dataset}')\n",
    "\n",
    "if config.dataset == 'cifar10':\n",
    "    num_classes = 10\n",
    "    input_size = 32\n",
    "    \n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    \n",
    "    trainset = dataset_utils.C10(root='~/datasets/CIFAR10', train=True, download=True, transform=transform_train)\n",
    "    testset = dataset_utils.C10(root='~/datasets/CIFAR10', train=False, download=True, transform=transform_test)\n",
    "\n",
    "elif config.dataset == 'svhn':\n",
    "    num_classes = 10\n",
    "    input_size = 32\n",
    "    \n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    trainset = dataset_utils.SVHN(root='~/datasets/SVHN', split='train', download=True, transform=transform_train)\n",
    "    testset = dataset_utils.SVHN(root='~/datasets/SVHN', split='test', download=True, transform=transform_test)\n",
    "\n",
    "# Create data loaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=config.batch_size_train, \n",
    "                                         shuffle=True, num_workers=config.num_workers)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=config.batch_size_test, \n",
    "                                        shuffle=False, num_workers=config.num_workers)\n",
    "\n",
    "print(f\"Training samples: {len(trainset)}\")\n",
    "print(f\"Test samples: {len(testset)}\")\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c801715e",
   "metadata": {},
   "source": [
    "## 4. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef1b9299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model: vgg16_bn\n",
      "Total parameters: 14.99M\n",
      "Loss function: sat\n",
      "Optimizer: SGD (lr=0.1, momentum=0.9, weight_decay=0.0005)\n",
      "Total parameters: 14.99M\n",
      "Loss function: sat\n",
      "Optimizer: SGD (lr=0.1, momentum=0.9, weight_decay=0.0005)\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "print(f\"Creating model: {config.arch}\")\n",
    "\n",
    "# Model has num_classes+1 outputs for selective classification (extra dimension for abstention)\n",
    "model_num_classes = num_classes if config.loss_type == 'ce' else num_classes + 1\n",
    "model = models.__dict__[config.arch](num_classes=model_num_classes, input_size=input_size)\n",
    "\n",
    "if use_cuda:\n",
    "    model = torch.nn.DataParallel(model.cuda())\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params/1e6:.2f}M\")\n",
    "\n",
    "# Setup loss function\n",
    "if config.loss_type == 'ce':\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "elif config.loss_type == 'gambler':\n",
    "    criterion = deep_gambler_loss\n",
    "elif config.loss_type == 'sat':\n",
    "    criterion = SelfAdativeTraining(num_examples=len(trainset), num_classes=num_classes, mom=config.sat_momentum)\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=config.lr, momentum=config.momentum, weight_decay=config.weight_decay)\n",
    "\n",
    "print(f\"Loss function: {config.loss_type}\")\n",
    "print(f\"Optimizer: SGD (lr={config.lr}, momentum={config.momentum}, weight_decay={config.weight_decay})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e841d530",
   "metadata": {},
   "source": [
    "## 5. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca62f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined.\n"
     ]
    }
   ],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, config):\n",
    "    \"\"\"Adjust learning rate according to schedule\"\"\"\n",
    "    if epoch in config.schedule:\n",
    "        config.lr *= config.gamma\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = config.lr\n",
    "        print(f\"Learning rate adjusted to: {config.lr}\")\n",
    "\n",
    "def train_epoch(trainloader, model, criterion, optimizer, epoch, use_cuda, config, verbose=False):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    \n",
    "    for batch_idx, batch_data in enumerate(trainloader):\n",
    "        inputs, targets, indices = batch_data\n",
    "        \n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        if epoch >= config.pretrain_epochs:\n",
    "            if config.loss_type == 'gambler':\n",
    "                loss = criterion(outputs, targets, config.reward)\n",
    "            elif config.loss_type == 'sat':\n",
    "                loss = criterion(outputs, targets, indices)\n",
    "            else:\n",
    "                loss = criterion(outputs, targets)\n",
    "        else:\n",
    "            # Pretrain with cross-entropy on class logits only\n",
    "            loss = F.cross_entropy(outputs[:, :-1], targets)\n",
    "        \n",
    "        # Measure accuracy\n",
    "        prec1 = accuracy(outputs.data, targets.data, topk=(1,))[0]\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return losses.avg, top1.avg\n",
    "\n",
    "def test_epoch(testloader, model, criterion, epoch, use_cuda, config):\n",
    "    \"\"\"Test for one epoch and evaluate coverage\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    abstention_results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(testloader):\n",
    "            inputs, targets, indices = batch_data\n",
    "            \n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            \n",
    "            outputs = model(inputs).cpu()\n",
    "            values, predictions = outputs.data.max(1)\n",
    "            \n",
    "            # Calculate loss and abstention results\n",
    "            if epoch >= config.pretrain_epochs:\n",
    "                if config.loss_type == 'gambler':\n",
    "                    loss = criterion(outputs, targets, config.reward)\n",
    "                elif config.loss_type == 'sat':\n",
    "                    loss = F.cross_entropy(outputs[:, :-1], targets)\n",
    "                else:\n",
    "                    loss = criterion(outputs, targets)\n",
    "                \n",
    "                outputs = F.softmax(outputs, dim=1)\n",
    "                outputs, reservation = outputs[:, :-1], outputs[:, -1]\n",
    "                abstention_results.extend(zip(list(reservation.numpy()), \n",
    "                                            list(predictions.eq(targets.data).numpy())))\n",
    "            else:\n",
    "                loss = F.cross_entropy(outputs[:, :-1], targets)\n",
    "            \n",
    "            prec1 = accuracy(outputs.data, targets.data, topk=(1,))[0]\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "    \n",
    "    # Calculate coverage-based accuracy\n",
    "    coverage_results = {}\n",
    "    if epoch >= config.pretrain_epochs and len(abstention_results) > 0:\n",
    "        abstention_results.sort(key=lambda x: x[0], reverse=True)\n",
    "        sorted_correct = list(map(lambda x: int(x[1]), abstention_results))\n",
    "        size = len(abstention_results)\n",
    "        \n",
    "        for coverage in config.expected_coverage:\n",
    "            num_samples = size - int(size * (100 - coverage) / 100)\n",
    "            if num_samples > 0:\n",
    "                acc = sum(sorted_correct[:num_samples]) / num_samples * 100\n",
    "                coverage_results[coverage] = acc\n",
    "    \n",
    "    return losses.avg, top1.avg, coverage_results\n",
    "\n",
    "print(\"Training functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be63ff4e",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dedf21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 3 epochs...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viet2005/workspace/Research/ltr_xai/SAT-Ensemble/venv/lib/python3.12/site-packages/torch/nn/functional.py:1531: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch [50/391] Loss: 1.8990 Acc: 30.47%\n"
     ]
    }
   ],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'epoch': [],\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': [],\n",
    "    'coverage_results': []\n",
    "}\n",
    "\n",
    "print(f\"Starting training for {config.epochs} epochs...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_acc = 0.0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Adjust learning rate (silently)\n",
    "    if epoch in config.schedule:\n",
    "        config.lr *= config.gamma\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = config.lr\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(trainloader, model, criterion, optimizer, epoch, use_cuda, config)\n",
    "    \n",
    "    # Test\n",
    "    test_loss, test_acc, coverage_results = test_epoch(testloader, model, criterion, epoch, use_cuda, config)\n",
    "    \n",
    "    # Save history\n",
    "    history['epoch'].append(epoch + 1)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    history['coverage_results'].append(coverage_results)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    # One-line summary\n",
    "    is_best = \"\"\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        checkpoint_path = os.path.join(config.save_dir, 'best_model.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_acc': best_acc,\n",
    "        }, checkpoint_path)\n",
    "        is_best = \" *BEST*\"\n",
    "    \n",
    "    # Compact one-line output\n",
    "    cov_str = \"\"\n",
    "    if coverage_results and 95 in coverage_results:\n",
    "        cov_str = f\" | Cov@95: {coverage_results[95]:.2f}%\"\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:3d}/{config.epochs} | LR: {config.lr:.6f} | Train: {train_acc:5.2f}% | Test: {test_acc:5.2f}%{cov_str} | {epoch_time:5.1f}s{is_best}\")\n",
    "    \n",
    "    # Save periodic checkpoint\n",
    "    if (epoch + 1) % 50 == 0 or (epoch + 1) == config.epochs:\n",
    "        checkpoint_path = os.path.join(config.save_dir, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "        torch.save(model, checkpoint_path)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Training completed in {total_time/3600:.2f} hours\")\n",
    "print(f\"Best test accuracy: {best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b86186a",
   "metadata": {},
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b7f73cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training curves saved to ./checkpoints/sat_notebook/training_curves.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10283/3914390843.py:24: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history['epoch'], history['train_loss'], label='Train Loss', marker='o', markersize=3)\n",
    "axes[0].plot(history['epoch'], history['test_loss'], label='Test Loss', marker='s', markersize=3)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Test Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(history['epoch'], history['train_acc'], label='Train Accuracy', marker='o', markersize=3)\n",
    "axes[1].plot(history['epoch'], history['test_acc'], label='Test Accuracy', marker='s', markersize=3)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training and Test Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(os.path.join(config.save_dir, 'training_curves.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training curves saved to {config.save_dir}/training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f46876d",
   "metadata": {},
   "source": [
    "## 8. Coverage vs Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "994e93c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Coverage vs Error Analysis:\n",
      " Coverage (%)  Accuracy (%)  Error (%)\n",
      "        100.0     32.600000  67.400000\n",
      "         99.0     32.222222  67.777778\n",
      "         98.0     31.887755  68.112245\n",
      "         97.0     31.628866  68.371134\n",
      "         95.0     31.336842  68.663158\n",
      "         90.0     30.544444  69.455556\n",
      "         85.0     29.717647  70.282353\n",
      "         80.0     28.925000  71.075000\n",
      "         75.0     28.373333  71.626667\n",
      "         70.0     27.785714  72.214286\n",
      "         60.0     26.400000  73.600000\n",
      "         50.0     25.480000  74.520000\n",
      "         40.0     24.775000  75.225000\n",
      "         30.0     24.633333  75.366667\n",
      "         20.0     25.150000  74.850000\n",
      "         10.0     23.200000  76.800000\n",
      "\n",
      "Results saved to ./checkpoints/sat_notebook/coverage_vs_error.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10283/2756235459.py:33: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Get final coverage results\n",
    "if history['coverage_results'][-1]:\n",
    "    final_coverage = history['coverage_results'][-1]\n",
    "    \n",
    "    coverages = sorted(final_coverage.keys(), reverse=True)\n",
    "    accuracies = [final_coverage[c] for c in coverages]\n",
    "    errors = [100 - acc for acc in accuracies]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Coverage (%)': coverages,\n",
    "        'Accuracy (%)': accuracies,\n",
    "        'Error (%)': errors\n",
    "    })\n",
    "    \n",
    "    print(\"\\nFinal Coverage vs Error Analysis:\")\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_path = os.path.join(config.save_dir, 'coverage_vs_error.csv')\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nResults saved to {csv_path}\")\n",
    "    \n",
    "    # Plot coverage vs error\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(coverages, errors, marker='o', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Coverage (%)', fontsize=12)\n",
    "    plt.ylabel('Error Rate (%)', fontsize=12)\n",
    "    plt.title('Coverage vs Error Rate (Final Epoch)', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.save_dir, 'coverage_vs_error.png'), dpi=150)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No coverage results available (model still in pretraining phase or using CE loss)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeaeeb9",
   "metadata": {},
   "source": [
    "## 9. Comprehensive Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdbaf235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model from epoch 3\n",
      "Best accuracy: 32.60%\n",
      "\n",
      "Running comprehensive evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viet2005/workspace/Research/ltr_xai/SAT-Ensemble/venv/lib/python3.12/site-packages/torch/nn/functional.py:1531: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Evaluation Results:\n",
      " Coverage (%)  Accuracy (%)  Error (%)  Num Samples\n",
      "          100     32.600000  67.400000        10000\n",
      "           99     32.727273  67.272727         9900\n",
      "           98     32.857143  67.142857         9800\n",
      "           97     33.000000  67.000000         9700\n",
      "           95     33.178947  66.821053         9500\n",
      "           90     33.644444  66.355556         9000\n",
      "           85     34.023529  65.976471         8500\n",
      "           80     34.462500  65.537500         8000\n",
      "           75     35.160000  64.840000         7500\n",
      "           70     36.014286  63.985714         7000\n",
      "           60     37.816667  62.183333         6000\n",
      "           50     39.720000  60.280000         5000\n",
      "           40     41.900000  58.100000         4000\n",
      "           30     43.833333  56.166667         3000\n",
      "           20     47.300000  52.700000         2000\n",
      "           10     51.100000  48.900000         1000\n",
      "\n",
      "Detailed evaluation saved to ./checkpoints/sat_notebook/detailed_evaluation.csv\n"
     ]
    }
   ],
   "source": [
    "def evaluate_with_abstention(model, testloader, use_cuda, num_classes, loss_type):\n",
    "    \"\"\"Comprehensive evaluation with abstention analysis\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    all_reservations = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data in testloader:\n",
    "            inputs, targets = batch_data[:2]\n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            outputs_soft = F.softmax(outputs, dim=1).cpu()\n",
    "            \n",
    "            if loss_type != 'ce':\n",
    "                class_probs = outputs_soft[:, :-1]\n",
    "                reservation = outputs_soft[:, -1]\n",
    "            else:\n",
    "                class_probs = outputs_soft\n",
    "                reservation = 1 - outputs_soft.max(1)[0]\n",
    "            \n",
    "            predictions = class_probs.max(1)[1]\n",
    "            \n",
    "            all_logits.append(class_probs.numpy())\n",
    "            all_labels.append(targets.numpy())\n",
    "            all_reservations.append(reservation.numpy())\n",
    "            all_predictions.append(predictions.numpy())\n",
    "    \n",
    "    all_logits = np.concatenate(all_logits)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_reservations = np.concatenate(all_reservations)\n",
    "    all_predictions = np.concatenate(all_predictions)\n",
    "    \n",
    "    # Calculate metrics at different coverage levels\n",
    "    coverages = [100, 99, 98, 97, 95, 90, 85, 80, 75, 70, 60, 50, 40, 30, 20, 10]\n",
    "    results = []\n",
    "    \n",
    "    # Sort by reservation (ascending - lower reservation = higher confidence)\n",
    "    sorted_indices = np.argsort(all_reservations)\n",
    "    \n",
    "    for coverage in coverages:\n",
    "        n_samples = int(len(all_labels) * coverage / 100)\n",
    "        if n_samples == 0:\n",
    "            continue\n",
    "        \n",
    "        # Take most confident samples\n",
    "        selected_indices = sorted_indices[:n_samples]\n",
    "        selected_predictions = all_predictions[selected_indices]\n",
    "        selected_labels = all_labels[selected_indices]\n",
    "        \n",
    "        accuracy = (selected_predictions == selected_labels).mean() * 100\n",
    "        error = 100 - accuracy\n",
    "        \n",
    "        results.append({\n",
    "            'Coverage (%)': coverage,\n",
    "            'Accuracy (%)': accuracy,\n",
    "            'Error (%)': error,\n",
    "            'Num Samples': n_samples\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Load best model\n",
    "best_checkpoint = os.path.join(config.save_dir, 'best_model.pth')\n",
    "if os.path.exists(best_checkpoint):\n",
    "    checkpoint = torch.load(best_checkpoint)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "    print(f\"Best accuracy: {checkpoint['best_acc']:.2f}%\")\n",
    "\n",
    "# Comprehensive evaluation\n",
    "print(\"\\nRunning comprehensive evaluation...\")\n",
    "eval_results = evaluate_with_abstention(model, testloader, use_cuda, num_classes, config.loss_type)\n",
    "print(\"\\nDetailed Evaluation Results:\")\n",
    "print(eval_results.to_string(index=False))\n",
    "\n",
    "# Save detailed results\n",
    "eval_csv_path = os.path.join(config.save_dir, 'detailed_evaluation.csv')\n",
    "eval_results.to_csv(eval_csv_path, index=False)\n",
    "print(f\"\\nDetailed evaluation saved to {eval_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce191f8",
   "metadata": {},
   "source": [
    "## 10. Summary and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b0282c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING SUMMARY\n",
      "================================================================================\n",
      "Dataset: cifar10\n",
      "Architecture: vgg16_bn\n",
      "Loss function: sat\n",
      "Total epochs: 3\n",
      "SAT Momentum: 0.99\n",
      "\n",
      "Final Training Accuracy: 28.40%\n",
      "Final Test Accuracy: 32.60%\n",
      "Best Test Accuracy: 32.60%\n",
      "\n",
      "================================================================================\n",
      "SELECTIVE CLASSIFICATION PERFORMANCE\n",
      "================================================================================\n",
      "Coverage 100% → Accuracy:  32.60% | Error: 67.40%\n",
      "Coverage  95% → Accuracy:  33.18% | Error: 66.82%\n",
      "Coverage  90% → Accuracy:  33.64% | Error: 66.36%\n",
      "Coverage  85% → Accuracy:  34.02% | Error: 65.98%\n",
      "Coverage  80% → Accuracy:  34.46% | Error: 65.54%\n",
      "Coverage  75% → Accuracy:  35.16% | Error: 64.84%\n",
      "Coverage  70% → Accuracy:  36.01% | Error: 63.99%\n",
      "\n",
      "================================================================================\n",
      "All results and checkpoints saved to: ./checkpoints/sat_notebook\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Print summary\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Dataset: {config.dataset}\")\n",
    "print(f\"Architecture: {config.arch}\")\n",
    "print(f\"Loss function: {config.loss_type}\")\n",
    "print(f\"Total epochs: {config.epochs}\")\n",
    "print(f\"SAT Momentum: {config.sat_momentum}\")\n",
    "print(f\"\\nFinal Training Accuracy: {history['train_acc'][-1]:.2f}%\")\n",
    "print(f\"Final Test Accuracy: {history['test_acc'][-1]:.2f}%\")\n",
    "print(f\"Best Test Accuracy: {best_acc:.2f}%\")\n",
    "\n",
    "if eval_results is not None and len(eval_results) > 0:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SELECTIVE CLASSIFICATION PERFORMANCE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for coverage in [100, 95, 90, 85, 80, 75, 70]:\n",
    "        row = eval_results[eval_results['Coverage (%)'] == coverage]\n",
    "        if len(row) > 0:\n",
    "            acc = row['Accuracy (%)'].values[0]\n",
    "            err = row['Error (%)'].values[0]\n",
    "            print(f\"Coverage {coverage:3.0f}% → Accuracy: {acc:6.2f}% | Error: {err:5.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"All results and checkpoints saved to:\", config.save_dir)\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
