{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ca4492e",
   "metadata": {},
   "source": [
    "# SAT on Long-Tailed CIFAR-10: Balanced and Worst-Group Analysis\n",
    "\n",
    "This notebook evaluates Self-Adaptive Training on imbalanced CIFAR-10 with:\n",
    "- **Balanced Error Rate**: Average error across all classes (unweighted)\n",
    "- **Worst-Group Error Rate**: Error of the worst-performing class\n",
    "- **AURC (Balanced)**: Area Under Risk-Coverage curve using balanced error\n",
    "- **AURC (Worst-Group)**: Area Under Risk-Coverage curve using worst-group error\n",
    "\n",
    "**Settings:**\n",
    "- Architecture: VGG-16 with Batch Normalization\n",
    "- Dataset: CIFAR-10 Long-Tailed (Imbalance Ratio configurable)\n",
    "- Loss: SAT with momentum 0.99\n",
    "- Training: 300 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28476d4f",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a404c724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from PIL import Image\n",
    "\n",
    "# For loading CIFAR-10-LT dataset\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Local imports\n",
    "import models.cifar as models\n",
    "from loss import SelfAdativeTraining\n",
    "from utils import AverageMeter, accuracy\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b7284e",
   "metadata": {},
   "source": [
    "## 2. Long-Tailed Dataset Loading with HuggingFace Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaadc6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper for CIFAR-10-LT to work with PyTorch DataLoader\n",
    "class CIFAR10LTWrapper(torch.utils.data.Dataset):\n",
    "    \"\"\"Wrapper for HuggingFace CIFAR-10-LT dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hf_dataset: HuggingFace dataset object\n",
    "            transform: torchvision transforms to apply\n",
    "        \"\"\"\n",
    "        self.dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "        self.targets = np.array([item['label'] for item in hf_dataset])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.dataset[index]\n",
    "        img = item['img']\n",
    "        target = item['label']\n",
    "        \n",
    "        # Convert to PIL Image if needed\n",
    "        if not isinstance(img, Image.Image):\n",
    "            img = Image.fromarray(np.array(img))\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, target, index\n",
    "    \n",
    "    def get_class_distribution(self):\n",
    "        \"\"\"Return distribution of samples per class\"\"\"\n",
    "        unique, counts = np.unique(self.targets, return_counts=True)\n",
    "        return dict(zip(unique, counts))\n",
    "\n",
    "print(\"CIFAR-10-LT wrapper class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8275fd7b",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba723e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    # Dataset settings\n",
    "    dataset = 'cifar10'\n",
    "    dataset_name = 'r-100'  # HuggingFace dataset name: r-10, r-20, r-50, r-100, r-200\n",
    "    imbalance_ratio = 100  # Majority/minority class ratio (10, 20, 50, 100, 200)\n",
    "    \n",
    "    # Architecture\n",
    "    arch = 'vgg16_bn'\n",
    "    \n",
    "    # Training settings\n",
    "    loss_type = 'sat'\n",
    "    epochs = 300\n",
    "    pretrain_epochs = 0\n",
    "    \n",
    "    # Hyperparameters (same as balanced SAT)\n",
    "    batch_size_train = 128\n",
    "    batch_size_test = 200\n",
    "    lr = 0.1\n",
    "    momentum = 0.9\n",
    "    sat_momentum = 0.99\n",
    "    weight_decay = 5e-4\n",
    "    gamma = 0.5\n",
    "    schedule = [25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275]\n",
    "    \n",
    "    # Evaluation settings\n",
    "    expected_coverage = [100., 99., 98., 97., 95., 90., 85., 80., 75., 70., 60., 50., 40., 30., 20., 10.]\n",
    "    \n",
    "    # System settings\n",
    "    gpu_id = '0'\n",
    "    num_workers = 4\n",
    "    manual_seed = 42\n",
    "    save_dir = f'./checkpoints/sat_longtailed_ir{100}'\n",
    "    \n",
    "config = Config()\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(config.manual_seed)\n",
    "torch.manual_seed(config.manual_seed)\n",
    "np.random.seed(config.manual_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(config.manual_seed)\n",
    "\n",
    "# Set GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = config.gpu_id\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Create save directory\n",
    "os.makedirs(config.save_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Dataset: Long-Tailed CIFAR-10 (dataset_name={config.dataset_name}, IR={config.imbalance_ratio})\")\n",
    "print(f\"  Architecture: {config.arch}\")\n",
    "print(f\"  Loss: {config.loss_type}\")\n",
    "print(f\"  Epochs: {config.epochs}\")\n",
    "print(f\"  SAT Momentum: {config.sat_momentum}\")\n",
    "print(f\"  Save directory: {config.save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34035f5",
   "metadata": {},
   "source": [
    "## 4. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e62a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "print(f'Preparing Long-Tailed CIFAR-10 from HuggingFace (dataset={config.dataset_name})...')\n",
    "\n",
    "num_classes = 10\n",
    "input_size = 32\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Load HuggingFace CIFAR-10-LT dataset\n",
    "print(f\"Loading dataset from cifar10-lt.py script...\")\n",
    "hf_train = load_dataset('./cifar10-lt.py', name=config.dataset_name, split='train')\n",
    "hf_test = load_dataset('./cifar10-lt.py', name=config.dataset_name, split='test')\n",
    "\n",
    "# Wrap with PyTorch-compatible wrapper\n",
    "trainset = CIFAR10LTWrapper(hf_train, transform=transform_train)\n",
    "testset = CIFAR10LTWrapper(hf_test, transform=transform_test)\n",
    "\n",
    "# Create data loaders\n",
    "trainloader = DataLoader(trainset, batch_size=config.batch_size_train, \n",
    "                        shuffle=True, num_workers=config.num_workers)\n",
    "testloader = DataLoader(testset, batch_size=config.batch_size_test, \n",
    "                       shuffle=False, num_workers=config.num_workers)\n",
    "\n",
    "print(f\"\\nTraining samples: {len(trainset)}\")\n",
    "print(f\"Test samples: {len(testset)}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Visualize distribution\n",
    "train_dist = trainset.get_class_distribution()\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.bar(range(num_classes), [train_dist[i] for i in range(num_classes)], alpha=0.7)\n",
    "ax.set_xlabel('Class', fontweight='bold')\n",
    "ax.set_ylabel('Number of Training Samples', fontweight='bold')\n",
    "ax.set_title(f'Long-Tailed CIFAR-10 Distribution (dataset={config.dataset_name}, IR={config.imbalance_ratio})', fontweight='bold')\n",
    "ax.set_xticks(range(num_classes))\n",
    "ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.save_dir, 'dataset_distribution.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nClass distribution saved to {config.save_dir}/dataset_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a902ed3",
   "metadata": {},
   "source": [
    "## 5. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28db4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "print(f\"Creating model: {config.arch}\")\n",
    "\n",
    "model_num_classes = num_classes + 1  # Extra dimension for abstention\n",
    "model = models.__dict__[config.arch](num_classes=model_num_classes, input_size=input_size)\n",
    "\n",
    "if use_cuda:\n",
    "    model = torch.nn.DataParallel(model.cuda())\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params/1e6:.2f}M\")\n",
    "\n",
    "# Setup loss function\n",
    "criterion = SelfAdativeTraining(num_examples=len(trainset), num_classes=num_classes, mom=config.sat_momentum)\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=config.lr, momentum=config.momentum, weight_decay=config.weight_decay)\n",
    "\n",
    "print(f\"Loss function: {config.loss_type}\")\n",
    "print(f\"Optimizer: SGD (lr={config.lr}, momentum={config.momentum}, weight_decay={config.weight_decay})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2c523c",
   "metadata": {},
   "source": [
    "## 6. Training Functions with Group-Aware Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b04926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(trainloader, model, criterion, optimizer, epoch, use_cuda, config):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    \n",
    "    # Track per-class accuracy\n",
    "    class_correct = np.zeros(num_classes)\n",
    "    class_total = np.zeros(num_classes)\n",
    "    \n",
    "    for batch_idx, batch_data in enumerate(trainloader):\n",
    "        inputs, targets, indices = batch_data\n",
    "        \n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        if epoch >= config.pretrain_epochs:\n",
    "            loss = criterion(outputs, targets, indices)\n",
    "        else:\n",
    "            loss = F.cross_entropy(outputs[:, :-1], targets)\n",
    "        \n",
    "        # Measure accuracy\n",
    "        prec1 = accuracy(outputs.data, targets.data, topk=(1,))[0]\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        \n",
    "        # Per-class statistics\n",
    "        _, predicted = outputs[:, :-1].max(1)\n",
    "        for i in range(num_classes):\n",
    "            mask = targets == i\n",
    "            if mask.sum() > 0:\n",
    "                class_correct[i] += (predicted[mask] == targets[mask]).sum().item()\n",
    "                class_total[i] += mask.sum().item()\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Calculate balanced accuracy\n",
    "    class_acc = []\n",
    "    for i in range(num_classes):\n",
    "        if class_total[i] > 0:\n",
    "            class_acc.append(100.0 * class_correct[i] / class_total[i])\n",
    "        else:\n",
    "            class_acc.append(0.0)\n",
    "    \n",
    "    balanced_acc = np.mean(class_acc)\n",
    "    worst_acc = np.min(class_acc)\n",
    "    \n",
    "    return losses.avg, top1.avg, balanced_acc, worst_acc\n",
    "\n",
    "def test_epoch(testloader, model, criterion, epoch, use_cuda, config):\n",
    "    \"\"\"Test for one epoch with group-aware metrics\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    \n",
    "    # Track per-class statistics\n",
    "    class_correct = np.zeros(num_classes)\n",
    "    class_total = np.zeros(num_classes)\n",
    "    \n",
    "    # Store predictions for coverage analysis\n",
    "    all_reservations = []\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_correct = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(testloader):\n",
    "            inputs, targets, indices = batch_data\n",
    "            \n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            \n",
    "            outputs = model(inputs).cpu()\n",
    "            \n",
    "            # Calculate loss\n",
    "            if epoch >= config.pretrain_epochs:\n",
    "                loss = F.cross_entropy(outputs[:, :-1], targets)\n",
    "                \n",
    "                # Get reservation scores\n",
    "                outputs_soft = F.softmax(outputs, dim=1)\n",
    "                class_probs = outputs_soft[:, :-1]\n",
    "                reservation = outputs_soft[:, -1]\n",
    "                \n",
    "                _, predictions = class_probs.max(1)\n",
    "            else:\n",
    "                loss = F.cross_entropy(outputs[:, :-1], targets)\n",
    "                _, predictions = outputs[:, :-1].max(1)\n",
    "                reservation = torch.zeros(len(targets))\n",
    "            \n",
    "            # Overall accuracy\n",
    "            prec1 = accuracy(outputs[:, :-1].data, targets.data, topk=(1,))[0]\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "            \n",
    "            # Per-class statistics\n",
    "            correct = predictions == targets\n",
    "            for i in range(num_classes):\n",
    "                mask = targets == i\n",
    "                if mask.sum() > 0:\n",
    "                    class_correct[i] += correct[mask].sum().item()\n",
    "                    class_total[i] += mask.sum().item()\n",
    "            \n",
    "            # Store for coverage analysis\n",
    "            all_reservations.extend(reservation.numpy().tolist())\n",
    "            all_predictions.extend(predictions.numpy().tolist())\n",
    "            all_targets.extend(targets.numpy().tolist())\n",
    "            all_correct.extend(correct.numpy().tolist())\n",
    "    \n",
    "    # Calculate balanced and worst-group accuracy\n",
    "    class_acc = []\n",
    "    for i in range(num_classes):\n",
    "        if class_total[i] > 0:\n",
    "            class_acc.append(100.0 * class_correct[i] / class_total[i])\n",
    "        else:\n",
    "            class_acc.append(0.0)\n",
    "    \n",
    "    balanced_acc = np.mean(class_acc)\n",
    "    worst_acc = np.min(class_acc)\n",
    "    \n",
    "    # Calculate coverage-based metrics\n",
    "    coverage_results = calculate_group_coverage_metrics(\n",
    "        np.array(all_reservations),\n",
    "        np.array(all_predictions),\n",
    "        np.array(all_targets),\n",
    "        np.array(all_correct),\n",
    "        config.expected_coverage\n",
    "    )\n",
    "    \n",
    "    return losses.avg, top1.avg, balanced_acc, worst_acc, class_acc, coverage_results\n",
    "\n",
    "def calculate_group_coverage_metrics(reservations, predictions, targets, correct, coverages):\n",
    "    \"\"\"Calculate balanced and worst-group error at different coverage levels\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Sort by reservation (high to low - high reservation = less confident = more likely to abstain)\n",
    "    sorted_indices = np.argsort(reservations)[::-1]\n",
    "    sorted_correct = correct[sorted_indices]\n",
    "    sorted_targets = targets[sorted_indices]\n",
    "    \n",
    "    for coverage in coverages:\n",
    "        # Select samples based on coverage (exclude high reservation samples)\n",
    "        n_samples = int(len(reservations) * coverage / 100)\n",
    "        if n_samples == 0:\n",
    "            continue\n",
    "        \n",
    "        # Take samples starting from lowest reservation (most confident)\n",
    "        selected_correct = sorted_correct[-n_samples:]\n",
    "        selected_targets = sorted_targets[-n_samples:]\n",
    "        \n",
    "        # Calculate per-class error\n",
    "        class_errors = []\n",
    "        for cls in range(num_classes):\n",
    "            mask = selected_targets == cls\n",
    "            if mask.sum() > 0:\n",
    "                cls_error = 100.0 * (1 - selected_correct[mask].mean())\n",
    "                class_errors.append(cls_error)\n",
    "            else:\n",
    "                class_errors.append(0.0)  # No samples for this class at this coverage\n",
    "        \n",
    "        balanced_error = np.mean(class_errors)\n",
    "        worst_error = np.max(class_errors)\n",
    "        overall_error = 100.0 * (1 - selected_correct.mean())\n",
    "        \n",
    "        results.append({\n",
    "            'coverage': coverage,\n",
    "            'overall_error': overall_error,\n",
    "            'balanced_error': balanced_error,\n",
    "            'worst_group_error': worst_error,\n",
    "            'class_errors': class_errors\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Training and evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eb34fb",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f8f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'epoch': [],\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'train_balanced_acc': [],\n",
    "    'train_worst_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': [],\n",
    "    'test_balanced_acc': [],\n",
    "    'test_worst_acc': [],\n",
    "    'test_class_acc': [],\n",
    "    'coverage_results': []\n",
    "}\n",
    "\n",
    "print(f\"Starting training for {config.epochs} epochs...\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"{'Epoch':>5} | {'LR':>10} | {'Train Acc':>10} | {'Bal Acc':>10} | {'Worst Acc':>10} | {'Test Acc':>10} | {'Bal Acc':>10} | {'Worst Acc':>10} | {'Time':>6} | {'Status'}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "best_balanced_acc = 0.0\n",
    "best_worst_acc = 0.0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Adjust learning rate\n",
    "    if epoch in config.schedule:\n",
    "        config.lr *= config.gamma\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = config.lr\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc, train_bal_acc, train_worst_acc = train_epoch(\n",
    "        trainloader, model, criterion, optimizer, epoch, use_cuda, config\n",
    "    )\n",
    "    \n",
    "    # Test\n",
    "    test_loss, test_acc, test_bal_acc, test_worst_acc, test_class_acc, coverage_results = test_epoch(\n",
    "        testloader, model, criterion, epoch, use_cuda, config\n",
    "    )\n",
    "    \n",
    "    # Save history\n",
    "    history['epoch'].append(epoch + 1)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_balanced_acc'].append(train_bal_acc)\n",
    "    history['train_worst_acc'].append(train_worst_acc)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    history['test_balanced_acc'].append(test_bal_acc)\n",
    "    history['test_worst_acc'].append(test_worst_acc)\n",
    "    history['test_class_acc'].append(test_class_acc)\n",
    "    history['coverage_results'].append(coverage_results)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    # Track best models\n",
    "    status = \"\"\n",
    "    if test_bal_acc > best_balanced_acc:\n",
    "        best_balanced_acc = test_bal_acc\n",
    "        checkpoint_path = os.path.join(config.save_dir, 'best_balanced_model.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'balanced_acc': best_balanced_acc,\n",
    "        }, checkpoint_path)\n",
    "        status += \"*BAL* \"\n",
    "    \n",
    "    if test_worst_acc > best_worst_acc:\n",
    "        best_worst_acc = test_worst_acc\n",
    "        checkpoint_path = os.path.join(config.save_dir, 'best_worst_model.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'worst_acc': best_worst_acc,\n",
    "        }, checkpoint_path)\n",
    "        status += \"*WORST*\"\n",
    "    \n",
    "    # Compact output\n",
    "    print(f\"{epoch+1:5d} | {config.lr:10.6f} | {train_acc:9.2f}% | {train_bal_acc:9.2f}% | {train_worst_acc:9.2f}% | \"\n",
    "          f\"{test_acc:9.2f}% | {test_bal_acc:9.2f}% | {test_worst_acc:9.2f}% | {epoch_time:5.1f}s | {status}\")\n",
    "    \n",
    "    # Save periodic checkpoint\n",
    "    if (epoch + 1) % 50 == 0 or (epoch + 1) == config.epochs:\n",
    "        checkpoint_path = os.path.join(config.save_dir, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "        torch.save(model, checkpoint_path)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(f\"Training completed in {total_time/3600:.2f} hours\")\n",
    "print(f\"Best balanced accuracy: {best_balanced_acc:.2f}%\")\n",
    "print(f\"Best worst-group accuracy: {best_worst_acc:.2f}%\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32311e17",
   "metadata": {},
   "source": [
    "## 8. Visualization: Training Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0168d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# 1. Overall accuracy\n",
    "ax = axes[0, 0]\n",
    "ax.plot(history['epoch'], history['train_acc'], label='Train', linewidth=2, marker='o', markersize=2)\n",
    "ax.plot(history['epoch'], history['test_acc'], label='Test', linewidth=2, marker='s', markersize=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('Overall Accuracy', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Balanced accuracy\n",
    "ax = axes[0, 1]\n",
    "ax.plot(history['epoch'], history['train_balanced_acc'], label='Train Balanced', linewidth=2, marker='o', markersize=2)\n",
    "ax.plot(history['epoch'], history['test_balanced_acc'], label='Test Balanced', linewidth=2, marker='s', markersize=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Balanced Accuracy (%)')\n",
    "ax.set_title('Balanced Accuracy (Average Across Classes)', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Worst-group accuracy\n",
    "ax = axes[1, 0]\n",
    "ax.plot(history['epoch'], history['train_worst_acc'], label='Train Worst', linewidth=2, marker='o', markersize=2, color='red')\n",
    "ax.plot(history['epoch'], history['test_worst_acc'], label='Test Worst', linewidth=2, marker='s', markersize=2, color='darkred')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Worst-Group Accuracy (%)')\n",
    "ax.set_title('Worst-Group Accuracy (Minimum Across Classes)', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. All metrics together\n",
    "ax = axes[1, 1]\n",
    "ax.plot(history['epoch'], history['test_acc'], label='Overall', linewidth=2)\n",
    "ax.plot(history['epoch'], history['test_balanced_acc'], label='Balanced', linewidth=2)\n",
    "ax.plot(history['epoch'], history['test_worst_acc'], label='Worst-Group', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Test Accuracy (%)')\n",
    "ax.set_title('Comparison of Test Metrics', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.save_dir, 'training_curves.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training curves saved to {config.save_dir}/training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1d91f7",
   "metadata": {},
   "source": [
    "## 9. Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03138b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze final per-class performance\n",
    "final_class_acc = history['test_class_acc'][-1]\n",
    "train_dist = trainset.get_class_distribution()\n",
    "\n",
    "# Create DataFrame\n",
    "df_class_perf = pd.DataFrame({\n",
    "    'Class': class_names,\n",
    "    'Train Samples': [train_dist[i] for i in range(num_classes)],\n",
    "    'Test Accuracy (%)': final_class_acc,\n",
    "    'Test Error (%)': [100 - acc for acc in final_class_acc]\n",
    "})\n",
    "\n",
    "print(\"\\nFinal Per-Class Performance:\")\n",
    "display(df_class_perf)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 1. Accuracy by class\n",
    "ax = axes[0]\n",
    "colors = plt.cm.RdYlGn(np.array(final_class_acc) / 100)\n",
    "bars = ax.bar(range(num_classes), final_class_acc, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('Class', fontweight='bold')\n",
    "ax.set_ylabel('Test Accuracy (%)', fontweight='bold')\n",
    "ax.set_title('Per-Class Test Accuracy', fontweight='bold', fontsize=14)\n",
    "ax.set_xticks(range(num_classes))\n",
    "ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "ax.axhline(y=np.mean(final_class_acc), color='blue', linestyle='--', linewidth=2, label=f'Balanced: {np.mean(final_class_acc):.1f}%')\n",
    "ax.axhline(y=np.min(final_class_acc), color='red', linestyle='--', linewidth=2, label=f'Worst: {np.min(final_class_acc):.1f}%')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, acc in enumerate(final_class_acc):\n",
    "    ax.text(i, acc + 1, f'{acc:.1f}', ha='center', fontsize=9)\n",
    "\n",
    "# 2. Training samples vs accuracy\n",
    "ax = axes[1]\n",
    "train_samples = [train_dist[i] for i in range(num_classes)]\n",
    "scatter = ax.scatter(train_samples, final_class_acc, s=200, alpha=0.6, c=range(num_classes), \n",
    "                     cmap='tab10', edgecolors='black', linewidth=2)\n",
    "ax.set_xlabel('Number of Training Samples', fontweight='bold')\n",
    "ax.set_ylabel('Test Accuracy (%)', fontweight='bold')\n",
    "ax.set_title('Training Sample Count vs Test Accuracy', fontweight='bold', fontsize=14)\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add class labels\n",
    "for i, name in enumerate(class_names):\n",
    "    ax.annotate(name, (train_samples[i], final_class_acc[i]), \n",
    "                fontsize=8, alpha=0.7, xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.save_dir, 'per_class_performance.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPer-class analysis saved to {config.save_dir}/per_class_performance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428a43bf",
   "metadata": {},
   "source": [
    "## 10. Coverage-Based Analysis: Balanced and Worst-Group Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6854271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract final coverage results\n",
    "final_coverage = history['coverage_results'][-1]\n",
    "\n",
    "# Create DataFrame\n",
    "df_coverage = pd.DataFrame(final_coverage)\n",
    "\n",
    "print(\"\\nCoverage-Based Error Analysis:\")\n",
    "print(\"=\"*80)\n",
    "display(df_coverage[['coverage', 'overall_error', 'balanced_error', 'worst_group_error']])\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 1. Coverage vs Error curves\n",
    "ax = axes[0]\n",
    "ax.plot(df_coverage['coverage'], df_coverage['overall_error'], \n",
    "        marker='o', markersize=8, linewidth=2.5, label='Overall Error', color='blue')\n",
    "ax.plot(df_coverage['coverage'], df_coverage['balanced_error'], \n",
    "        marker='s', markersize=8, linewidth=2.5, label='Balanced Error', color='green')\n",
    "ax.plot(df_coverage['coverage'], df_coverage['worst_group_error'], \n",
    "        marker='^', markersize=8, linewidth=2.5, label='Worst-Group Error', color='red')\n",
    "ax.set_xlabel('Coverage (%)', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Error Rate (%)', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Selective Classification: Coverage vs Error (Long-Tailed)', fontweight='bold', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.invert_xaxis()\n",
    "\n",
    "# Highlight key coverage points\n",
    "for cov in [100, 95, 90, 80]:\n",
    "    row = df_coverage[df_coverage['coverage'] == cov]\n",
    "    if len(row) > 0:\n",
    "        bal_err = row['balanced_error'].values[0]\n",
    "        worst_err = row['worst_group_error'].values[0]\n",
    "        ax.annotate(f'B:{bal_err:.1f}%\\nW:{worst_err:.1f}%', \n",
    "                    xy=(cov, worst_err), xytext=(10, 10), \n",
    "                    textcoords='offset points', fontsize=8,\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.5))\n",
    "\n",
    "# 2. Error reduction comparison\n",
    "ax = axes[1]\n",
    "overall_reduction = df_coverage['overall_error'].iloc[0] - df_coverage['overall_error']\n",
    "balanced_reduction = df_coverage['balanced_error'].iloc[0] - df_coverage['balanced_error']\n",
    "worst_reduction = df_coverage['worst_group_error'].iloc[0] - df_coverage['worst_group_error']\n",
    "\n",
    "x = np.arange(len(df_coverage))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, overall_reduction, width, label='Overall', alpha=0.8, color='blue')\n",
    "ax.bar(x, balanced_reduction, width, label='Balanced', alpha=0.8, color='green')\n",
    "ax.bar(x + width, worst_reduction, width, label='Worst-Group', alpha=0.8, color='red')\n",
    "\n",
    "ax.set_xlabel('Coverage Level', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Error Reduction from 100% Coverage (%)', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Error Reduction at Different Coverage Levels', fontweight='bold', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"{int(c)}%\" for c in df_coverage['coverage']], rotation=45)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.save_dir, 'coverage_analysis.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCoverage analysis saved to {config.save_dir}/coverage_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7223394e",
   "metadata": {},
   "source": [
    "## 11. AURC Calculation: Balanced and Worst-Group Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d97b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_aurc(coverages, errors):\n",
    "    \"\"\"\n",
    "    Calculate Area Under the Risk-Coverage curve\n",
    "    AURC = integral of error rate over coverage\n",
    "    Lower is better\n",
    "    \"\"\"\n",
    "    # Sort by coverage (descending)\n",
    "    sorted_indices = np.argsort(coverages)[::-1]\n",
    "    sorted_cov = np.array(coverages)[sorted_indices]\n",
    "    sorted_err = np.array(errors)[sorted_indices]\n",
    "    \n",
    "    # Calculate area using trapezoidal rule\n",
    "    # Normalize coverage to [0, 1]\n",
    "    cov_normalized = sorted_cov / 100.0\n",
    "    err_normalized = sorted_err / 100.0\n",
    "    \n",
    "    aurc = np.trapz(err_normalized, cov_normalized)\n",
    "    return aurc\n",
    "\n",
    "# Calculate AURC for different metrics\n",
    "coverages = df_coverage['coverage'].values\n",
    "overall_errors = df_coverage['overall_error'].values\n",
    "balanced_errors = df_coverage['balanced_error'].values\n",
    "worst_errors = df_coverage['worst_group_error'].values\n",
    "\n",
    "aurc_overall = calculate_aurc(coverages, overall_errors)\n",
    "aurc_balanced = calculate_aurc(coverages, balanced_errors)\n",
    "aurc_worst = calculate_aurc(coverages, worst_errors)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AURC (Area Under Risk-Coverage Curve) - Lower is Better\")\n",
    "print(\"=\"*80)\n",
    "print(f\"AURC (Overall):     {aurc_overall:.6f}\")\n",
    "print(f\"AURC (Balanced):    {aurc_balanced:.6f}\")\n",
    "print(f\"AURC (Worst-Group): {aurc_worst:.6f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Visualize AURC\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (ax, errors, title, color, aurc) in enumerate([\n",
    "    (axes[0], overall_errors, 'Overall Error', 'blue', aurc_overall),\n",
    "    (axes[1], balanced_errors, 'Balanced Error', 'green', aurc_balanced),\n",
    "    (axes[2], worst_errors, 'Worst-Group Error', 'red', aurc_worst)\n",
    "]):\n",
    "    # Normalize for visualization\n",
    "    cov_norm = coverages / 100.0\n",
    "    err_norm = errors / 100.0\n",
    "    \n",
    "    # Plot curve\n",
    "    ax.plot(cov_norm, err_norm, linewidth=3, color=color, marker='o', markersize=6)\n",
    "    \n",
    "    # Fill area under curve\n",
    "    ax.fill_between(cov_norm, 0, err_norm, alpha=0.3, color=color)\n",
    "    \n",
    "    ax.set_xlabel('Coverage (fraction)', fontweight='bold', fontsize=11)\n",
    "    ax.set_ylabel('Risk (error rate)', fontweight='bold', fontsize=11)\n",
    "    ax.set_title(f'{title}\\nAURC = {aurc:.6f}', fontweight='bold', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, max(err_norm) * 1.1])\n",
    "    \n",
    "    # Add text box with AURC value\n",
    "    ax.text(0.05, 0.95, f'AURC: {aurc:.6f}', transform=ax.transAxes,\n",
    "            verticalalignment='top', fontsize=12, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.save_dir, 'aurc_analysis.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAURC analysis saved to {config.save_dir}/aurc_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e16ecb",
   "metadata": {},
   "source": [
    "## 12. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb38f361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"SAT ON LONG-TAILED CIFAR-10 - FINAL REPORT (IR={config.imbalance_ratio})\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\n1. OVERALL PERFORMANCE:\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"Final Test Accuracy (Overall):     {history['test_acc'][-1]:.2f}%\")\n",
    "print(f\"Final Test Accuracy (Balanced):    {history['test_balanced_acc'][-1]:.2f}%\")\n",
    "print(f\"Final Test Accuracy (Worst-Group): {history['test_worst_acc'][-1]:.2f}%\")\n",
    "print(f\"\\nBest Balanced Accuracy:             {best_balanced_acc:.2f}%\")\n",
    "print(f\"Best Worst-Group Accuracy:          {best_worst_acc:.2f}%\")\n",
    "\n",
    "print(\"\\n2. PER-CLASS ACCURACY:\")\n",
    "print(\"-\" * 100)\n",
    "for i, (name, acc, samples) in enumerate(zip(class_names, final_class_acc, [train_dist[i] for i in range(num_classes)])):\n",
    "    print(f\"Class {i} ({name:12s}): {acc:6.2f}%  (train samples: {samples:5d})\")\n",
    "\n",
    "print(\"\\n3. COVERAGE-BASED ANALYSIS (Key Coverage Levels):\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"{'Coverage':<10} | {'Overall Err':<12} | {'Balanced Err':<13} | {'Worst Err':<10} | {'Bal Reduction':<14} | {'Worst Reduction'}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for cov in [100, 99, 95, 90, 85, 80, 75, 70]:\n",
    "    row = df_coverage[df_coverage['coverage'] == cov]\n",
    "    if len(row) > 0:\n",
    "        overall_err = row['overall_error'].values[0]\n",
    "        bal_err = row['balanced_error'].values[0]\n",
    "        worst_err = row['worst_group_error'].values[0]\n",
    "        \n",
    "        bal_reduction = df_coverage['balanced_error'].iloc[0] - bal_err\n",
    "        worst_reduction = df_coverage['worst_group_error'].iloc[0] - worst_err\n",
    "        \n",
    "        print(f\"{cov:<10.0f} | {overall_err:<12.2f} | {bal_err:<13.2f} | {worst_err:<10.2f} | \"\n",
    "              f\"{bal_reduction:<14.2f} | {worst_reduction:.2f}\")\n",
    "\n",
    "print(\"\\n4. AURC (Area Under Risk-Coverage Curve):\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"AURC (Overall):     {aurc_overall:.6f}\")\n",
    "print(f\"AURC (Balanced):    {aurc_balanced:.6f}  [Balanced risk metric]\")\n",
    "print(f\"AURC (Worst-Group): {aurc_worst:.6f}  [Worst-case risk metric]\")\n",
    "\n",
    "print(\"\\n5. KEY INSIGHTS:\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Calculate insights\n",
    "bal_gap = history['test_acc'][-1] - history['test_balanced_acc'][-1]\n",
    "worst_gap = history['test_acc'][-1] - history['test_worst_acc'][-1]\n",
    "class_variance = np.var(final_class_acc)\n",
    "\n",
    "# Error reduction at 80% coverage\n",
    "row_100 = df_coverage[df_coverage['coverage'] == 100].iloc[0]\n",
    "row_80 = df_coverage[df_coverage['coverage'] == 80].iloc[0]\n",
    "bal_reduction_80 = row_100['balanced_error'] - row_80['balanced_error']\n",
    "worst_reduction_80 = row_100['worst_group_error'] - row_80['worst_group_error']\n",
    "\n",
    "print(f\"• Performance Gap (Overall vs Balanced):   {bal_gap:.2f}%\")\n",
    "print(f\"• Performance Gap (Overall vs Worst):     {worst_gap:.2f}%\")\n",
    "print(f\"• Class Accuracy Variance:                {class_variance:.2f}\")\n",
    "print(f\"• Balanced Error Reduction (100% → 80%):  {bal_reduction_80:.2f}%\")\n",
    "print(f\"• Worst Error Reduction (100% → 80%):     {worst_reduction_80:.2f}%\")\n",
    "\n",
    "if bal_gap > 5:\n",
    "    print(f\"\\n⚠️  Large gap between overall and balanced accuracy suggests class imbalance impact\")\n",
    "if worst_gap > 10:\n",
    "    print(f\"⚠️  Very large gap to worst-group indicates severe performance disparity\")\n",
    "if worst_reduction_80 < bal_reduction_80:\n",
    "    print(f\"⚠️  Selective classification less effective for worst-performing groups\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "# Save report to file\n",
    "report_path = os.path.join(config.save_dir, 'summary_report.txt')\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(f\"SAT ON LONG-TAILED CIFAR-10 - SUMMARY REPORT (IR={config.imbalance_ratio})\\n\")\n",
    "    f.write(\"=\"*100 + \"\\n\\n\")\n",
    "    f.write(f\"Final Test Accuracy (Overall):     {history['test_acc'][-1]:.2f}%\\n\")\n",
    "    f.write(f\"Final Test Accuracy (Balanced):    {history['test_balanced_acc'][-1]:.2f}%\\n\")\n",
    "    f.write(f\"Final Test Accuracy (Worst-Group): {history['test_worst_acc'][-1]:.2f}%\\n\\n\")\n",
    "    f.write(f\"AURC (Overall):     {aurc_overall:.6f}\\n\")\n",
    "    f.write(f\"AURC (Balanced):    {aurc_balanced:.6f}\\n\")\n",
    "    f.write(f\"AURC (Worst-Group): {aurc_worst:.6f}\\n\")\n",
    "\n",
    "print(f\"\\nSummary report saved to: {report_path}\")\n",
    "\n",
    "# Save coverage results to CSV\n",
    "csv_path = os.path.join(config.save_dir, 'coverage_results.csv')\n",
    "df_coverage[['coverage', 'overall_error', 'balanced_error', 'worst_group_error']].to_csv(csv_path, index=False)\n",
    "print(f\"Coverage results saved to: {csv_path}\")\n",
    "\n",
    "print(\"\\n✅ Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
